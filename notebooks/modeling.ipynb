{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pformat\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "from narrative_time import conversion_utils\n",
    "from narrative_time import modeling_utils\n",
    "from narrative_time import event_relations\n",
    "from narrative_time.modeling import TransformerForRelationPrediction\n",
    "\n",
    "NUM_RELATIONS = len(event_relations.REL_TO_ID)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file contains only the code to train the model and compute simple metrics. If you want to reproduce the plots, they are avaialble in the notebook `modeling_and_plotting.ipynb` ðŸ˜‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"google/long-t5-tglobal-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "a1_annotations = conversion_utils.get_annotations(\"../corpus/timebank/nt_format/tbd_a1.jsonl\", as_dict=True)\n",
    "a2_annotations = conversion_utils.get_annotations(\"../corpus/timebank/nt_format/tbd_a2.jsonl\", as_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_DOCUMENTS = [\n",
    "    \"PRI19980115.2000.0186\",\n",
    "    \"PRI19980213.2000.0313\",\n",
    "    \"PRI19980121.2000.2591\",\n",
    "    \"ABC19980114.1830.0611\",\n",
    "    \"APW19980213.1380\",\n",
    "    \"NYT19980402.0453\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = []\n",
    "test_dataset = []\n",
    "\n",
    "n_errors = 0\n",
    "for annotation_id, annotation in a1_annotations.items():\n",
    "    annotation = modeling_utils.NTAnnotation.from_json(annotation)\n",
    "    input_ids, event_left_tokens, event_relation_matrix = modeling_utils.preprocess_document(annotation, tokenizer)\n",
    "\n",
    "    if annotation_id in TEST_DOCUMENTS:\n",
    "        test_dataset.append((input_ids, event_left_tokens, event_relation_matrix))\n",
    "    else:\n",
    "        train_dataset.append((input_ids, event_left_tokens, event_relation_matrix))\n",
    "\n",
    "assert len(test_dataset) == len(TEST_DOCUMENTS)\n",
    "\n",
    "for annotation_id, annotation in a2_annotations.items():\n",
    "    annotation = modeling_utils.NTAnnotation.from_json(annotation)\n",
    "    input_ids, event_left_tokens, event_relation_matrix = modeling_utils.preprocess_document(annotation, tokenizer)\n",
    "\n",
    "    if annotation_id in TEST_DOCUMENTS:\n",
    "        test_dataset.append((input_ids, event_left_tokens, event_relation_matrix))\n",
    "    else:\n",
    "        train_dataset.append((input_ids, event_left_tokens, event_relation_matrix))\n",
    "\n",
    "assert len(test_dataset) == len(TEST_DOCUMENTS) * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACCUM_STEPS = 1\n",
    "EARLY_STOPPING = -1\n",
    "\n",
    "dtype = torch.bfloat16\n",
    "# bfloat is only supported on 30-series and A-series GPUs (released 2020)\n",
    "# if you have older ones try float16\n",
    "# but fp16 usually requires some extra tricks that we didn't implement here\n",
    "# worst case, you can use float32, but it will be slower and use more memory\n",
    "model = TransformerForRelationPrediction(MODEL_NAME, num_relations=NUM_RELATIONS).to(device=\"cuda\", dtype=dtype)\n",
    "model.transformer.gradient_checkpointing_enable()  # needed to fit bigger documents into GPU memory\n",
    "\n",
    "_n_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Number of model parameters: {_n_params/1e6:.2f}M\")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)#, weight_decay=1e-2)\n",
    "\n",
    "wandb.init(\n",
    "    project=\"narrative-time\",\n",
    "    config={\n",
    "        \"model_name\": MODEL_NAME,\n",
    "        \"gradient_accumulation_steps\": ACCUM_STEPS,\n",
    "    },\n",
    ")\n",
    "wandb.watch(model)\n",
    "\n",
    "global_step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = None  # just here for linter to be happy\n",
    "early_stopping = EARLY_STOPPING\n",
    "best_f1 = 0\n",
    "best_epoch = 0\n",
    "\n",
    "for epoch in tqdm(range(30)):\n",
    "    shuffled_train_dataset = train_dataset.copy()\n",
    "    np.random.shuffle(shuffled_train_dataset)\n",
    "\n",
    "    for input_ids, event_left_tokens, event_relation_matrix in shuffled_train_dataset:\n",
    "        input_ids = input_ids.to(\"cuda\")\n",
    "        event_left_tokens = event_left_tokens.to(\"cuda\")\n",
    "        event_relation_matrix = event_relation_matrix.to(\"cuda\")\n",
    "\n",
    "        relation_logits = model(input_ids, event_left_tokens)\n",
    "\n",
    "        num_events = relation_logits.shape[0]\n",
    "        relation_logits = relation_logits.view(num_events * num_events, NUM_RELATIONS)\n",
    "        targets = event_relation_matrix.view(num_events * num_events)\n",
    "\n",
    "        loss = F.cross_entropy(relation_logits, targets, ignore_index=-1)\n",
    "        loss /= ACCUM_STEPS\n",
    "        loss.backward()\n",
    "\n",
    "        if global_step % ACCUM_STEPS == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            accuracy = (relation_logits.argmax(dim=1) == targets).float().mean()\n",
    "            wandb.log({\"loss\": loss.item(), \"train_accuracy\": accuracy.item(), \"epoch\": epoch}, step=global_step)\n",
    "\n",
    "        global_step += 1\n",
    "\n",
    "    # evaluate\n",
    "    model.eval()\n",
    "\n",
    "    all_predictions_list = []\n",
    "    all_targets_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_ids, event_left_tokens, event_relation_matrix in test_dataset:\n",
    "            input_ids = input_ids.to(\"cuda\")\n",
    "            event_left_tokens = event_left_tokens.to(\"cuda\")\n",
    "            event_relation_matrix = event_relation_matrix.to(\"cuda\")\n",
    "\n",
    "            relation_logits = model(input_ids, event_left_tokens)\n",
    "\n",
    "            num_events = relation_logits.shape[0]\n",
    "            relation_logits = relation_logits.view(num_events * num_events, NUM_RELATIONS)\n",
    "            targets = event_relation_matrix.view(num_events * num_events)\n",
    "\n",
    "            loss = F.cross_entropy(relation_logits, targets, ignore_index=-1)\n",
    "            accuracy = (relation_logits.argmax(dim=1) == targets).float().mean()\n",
    "\n",
    "            all_predictions_list.append(relation_logits.argmax(dim=1).cpu())\n",
    "            all_targets_list.append(targets.cpu())\n",
    "\n",
    "    all_predictions = torch.cat(all_predictions_list)\n",
    "    all_targets = torch.cat(all_targets_list)\n",
    "\n",
    "    # -1 are on the main diagonal (SELF relation) and are ignored\n",
    "    all_predictions = all_predictions[all_targets != -1]\n",
    "    all_targets = all_targets[all_targets != -1]\n",
    "\n",
    "    accuracy = (all_predictions == all_targets).float().mean()\n",
    "    p = precision_score(all_targets, all_predictions, average=\"macro\", zero_division=0)\n",
    "    r = recall_score(all_targets, all_predictions, average=\"macro\", zero_division=0)\n",
    "    f1 = f1_score(all_targets, all_predictions, average=\"macro\", zero_division=0)\n",
    "\n",
    "    metrics = {\n",
    "        \"test_accuracy\": accuracy.item(),\n",
    "        \"test_precision_macro\": p,\n",
    "        \"test_recall_macro\": r,\n",
    "        \"test_f1_macro\": f1,\n",
    "    }\n",
    "\n",
    "    wandb.log(metrics, step=global_step)\n",
    "    model.train()\n",
    "\n",
    "    # early stopping\n",
    "    if EARLY_STOPPING > 0:\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_epoch = epoch\n",
    "            early_stopping = EARLY_STOPPING\n",
    "\n",
    "            # save model\n",
    "            checkpoint = {\n",
    "                \"model\": model.state_dict(),\n",
    "                \"epoch\": epoch,\n",
    "                \"global_step\": global_step,\n",
    "                \"metrics\": metrics,\n",
    "            }\n",
    "            torch.save(checkpoint, \"best_model.pt\")\n",
    "            del checkpoint\n",
    "        else:\n",
    "            early_stopping -= 1\n",
    "        \n",
    "        if early_stopping == 0:\n",
    "            break\n",
    "\n",
    "print(f\"Best metrics:\\n{pformat(metrics)}\")\n",
    "if EARLY_STOPPING > 0:\n",
    "    # load best model\n",
    "    checkpoint = torch.load(\"best_model.pt\")\n",
    "    model.load_state_dict(checkpoint[\"model\"])\n",
    "\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (main, Nov 24 2022, 14:13:03) [GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2e765dc32e9506bd7edda5ccb3221d2f22a5a7d147137a461dec8148a5cc552b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
