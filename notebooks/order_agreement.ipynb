{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import krippendorff\n",
    "from sklearn import metrics\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "REL_TO_ID = {\n",
    "    \"BEFORE\": 0,\n",
    "    \"AFTER\": 1,\n",
    "    \"INCLUDES\": 2,\n",
    "    \"IS_INCLUDED\": 3,\n",
    "    \"SIMULTANEOUS\": 4,\n",
    "    \"OVERLAP\": 5,\n",
    "    \"VAGUE\": 6,\n",
    "}\n",
    "\n",
    "\n",
    "def make_event_vocab(soup):\n",
    "    \"\"\"Make a vocabulary of events/timexes.\n",
    "\n",
    "    Extracts all eeid (from MAKEINSTANCE) and tid (from TIMEX3) and creates a\n",
    "    vocabulary of events/timexes.\n",
    "\n",
    "    Returns:\n",
    "        event_vocab: a dictionary mapping event/timex ids to indices in the graph\n",
    "    \"\"\"\n",
    "    # extract all eeid and tid\n",
    "    eeid = set()\n",
    "    tid = set()\n",
    "    for elem in soup.find_all(\"MAKEINSTANCE\"):\n",
    "        eeid.add(elem[\"eiid\"])\n",
    "    for elem in soup.find_all(\"TIMEX3\"):\n",
    "        tid.add(elem[\"tid\"])\n",
    "\n",
    "    # create a vocabulary of events/timexes\n",
    "    event_vocab = {}\n",
    "    for i, e in enumerate(eeid.union(tid)):\n",
    "        event_vocab[e] = i\n",
    "    return event_vocab\n",
    "\n",
    "\n",
    "def make_graph(soup, event_vocab=None):\n",
    "    \"\"\"Make a graph from TML soup object.\n",
    "\n",
    "    Extracts all eeid (from MAKEINSTANCE) and tid (from TIMEX3) and creates a\n",
    "    vocabulary of events/timexes. Then, for each event, it extracts all\n",
    "    relations (from TLINK) and creates a graph.\n",
    "\n",
    "    Args:\n",
    "        soup: a soup object of a TML file\n",
    "        event_vocab: (optional) a dictionary mapping event/timex ids to indices in the graph\n",
    "\n",
    "    Returns:\n",
    "        graph: a numpy array of shape (n_events, n_events)\n",
    "        event_vocab: a dictionary mapping event/timex ids to indices in the graph\n",
    "    \"\"\"\n",
    "    if event_vocab is None:\n",
    "        event_vocab = make_event_vocab(soup)\n",
    "\n",
    "    # create a graph\n",
    "    n_events = len(event_vocab)\n",
    "    tlinks = soup.find_all(\"TLINK\")\n",
    "    if len(tlinks) != n_events ** 2 - n_events:\n",
    "        raise RuntimeError(f\"Number of TLINKs ({len(tlinks)}) does not make a full graph out of {n_events} events ({n_events ** 2 - n_events})\")\n",
    "\n",
    "    graph = -1 * np.ones((n_events, n_events), dtype=np.int8)\n",
    "    for tlink in tlinks:\n",
    "        left = tlink.get(\"eventInstanceID\")\n",
    "        if left is None:\n",
    "            left = tlink.get(\"timeID\")\n",
    "\n",
    "        if left is None:\n",
    "            raise RuntimeError(tlink)\n",
    "\n",
    "        right = tlink.get(\"relatedToEventInstance\")\n",
    "        if right is None:\n",
    "            right = tlink.get(\"relatedToTime\")\n",
    "\n",
    "        if right is None:\n",
    "            raise RuntimeError(tlink)\n",
    "\n",
    "        assert graph[event_vocab[left], event_vocab[right]] == -1\n",
    "        graph[event_vocab[left], event_vocab[right]] = REL_TO_ID[tlink[\"relType\"]]\n",
    "\n",
    "    # this approach to error is better for debugging\n",
    "    error = None\n",
    "    if np.any((graph + np.identity(n_events)) == -1):\n",
    "        error = \"Some relations are missing\"\n",
    "\n",
    "    return graph, event_vocab, error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Usage For a single file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../corpus/timeml_converted/a1/ABC19980108.1830.0711.tml\", \"r\") as f:\n",
    "    soup1 = BeautifulSoup(f.read(), \"xml\")\n",
    "\n",
    "with open(\"../corpus/timeml_converted/a2/ABC19980108.1830.0711.tml\", \"r\") as f:\n",
    "    soup2 = BeautifulSoup(f.read(), \"xml\")\n",
    "\n",
    "graph1, vocab, err = make_graph(soup1)\n",
    "assert err is None\n",
    "graph2, _, err = make_graph(soup2, vocab)\n",
    "assert err is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.92138671875\n",
      "Kappa   :  0.8992956947449614\n",
      "Krippendorff alpha:  0.8991251373618178\n"
     ]
    }
   ],
   "source": [
    "flat_graph1 = graph1.flatten()\n",
    "flat_graph2 = graph2.flatten()\n",
    "\n",
    "# compute agreement\n",
    "print(\"Accuracy: \", metrics.accuracy_score(flat_graph1, flat_graph2))\n",
    "print(\"Kappa   : \", metrics.cohen_kappa_score(flat_graph1, flat_graph2))\n",
    "print(\"Krippendorff alpha: \", krippendorff.alpha([flat_graph1, flat_graph2], level_of_measurement=\"nominal\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph1, vocab1, err = make_graph(soup1)\n",
    "assert err is None\n",
    "graph2, vocab2, err = make_graph(soup2)\n",
    "assert err is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab1 - Vocab2:  set()\n",
      "Vocab2 - Vocab1:  set()\n"
     ]
    }
   ],
   "source": [
    "# difference between vocab1 and vocab2 keys\n",
    "print(\"Vocab1 - Vocab2: \", set(vocab1.keys()) - set(vocab2.keys()))\n",
    "print(\"Vocab2 - Vocab1: \", set(vocab2.keys()) - set(vocab1.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bc0e2e5332644d5b1c1c6f24edc8a60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../corpus/timeml_converted/a1/ABC19980108.1830.0711.tml ../corpus/timeml_converted/a2/ABC19980108.1830.0711.tml\n",
      "../corpus/timeml_converted/a1/ABC19980114.1830.0611.tml ../corpus/timeml_converted/a2/ABC19980114.1830.0611.tml\n",
      "../corpus/timeml_converted/a1/ABC19980120.1830.0957.tml ../corpus/timeml_converted/a2/ABC19980120.1830.0957.tml\n",
      "../corpus/timeml_converted/a1/ABC19980304.1830.1636.tml ../corpus/timeml_converted/a2/ABC19980304.1830.1636.tml\n",
      "../corpus/timeml_converted/a1/AP900815-0044.tml ../corpus/timeml_converted/a2/AP900815-0044.tml\n",
      "../corpus/timeml_converted/a1/AP900816-0139.tml ../corpus/timeml_converted/a2/AP900816-0139.tml\n",
      "../corpus/timeml_converted/a1/APW19980213.1310.tml ../corpus/timeml_converted/a2/APW19980213.1310.tml\n",
      "../corpus/timeml_converted/a1/APW19980213.1320.tml ../corpus/timeml_converted/a2/APW19980213.1320.tml\n",
      "../corpus/timeml_converted/a1/APW19980213.1380.tml ../corpus/timeml_converted/a2/APW19980213.1380.tml\n",
      "../corpus/timeml_converted/a1/APW19980219.0476.tml ../corpus/timeml_converted/a2/APW19980219.0476.tml\n",
      "../corpus/timeml_converted/a1/APW19980227.0468.tml ../corpus/timeml_converted/a2/APW19980227.0468.tml\n",
      "../corpus/timeml_converted/a1/APW19980227.0476.tml ../corpus/timeml_converted/a2/APW19980227.0476.tml\n",
      "../corpus/timeml_converted/a1/APW19980227.0487.tml ../corpus/timeml_converted/a2/APW19980227.0487.tml\n",
      "../corpus/timeml_converted/a1/APW19980227.0489.tml ../corpus/timeml_converted/a2/APW19980227.0489.tml\n",
      "../corpus/timeml_converted/a1/APW19980227.0494.tml ../corpus/timeml_converted/a2/APW19980227.0494.tml\n",
      "../corpus/timeml_converted/a1/APW19980308.0201.tml ../corpus/timeml_converted/a2/APW19980308.0201.tml\n",
      "../corpus/timeml_converted/a1/APW19980418.0210.tml ../corpus/timeml_converted/a2/APW19980418.0210.tml\n",
      "../corpus/timeml_converted/a1/CNN19980126.1600.1104.tml ../corpus/timeml_converted/a2/CNN19980126.1600.1104.tml\n",
      "../corpus/timeml_converted/a1/CNN19980213.2130.0155.tml ../corpus/timeml_converted/a2/CNN19980213.2130.0155.tml\n",
      "../corpus/timeml_converted/a1/CNN19980222.1130.0084.tml ../corpus/timeml_converted/a2/CNN19980222.1130.0084.tml\n",
      "../corpus/timeml_converted/a1/CNN19980223.1130.0960.tml ../corpus/timeml_converted/a2/CNN19980223.1130.0960.tml\n",
      "../corpus/timeml_converted/a1/CNN19980227.2130.0067.tml ../corpus/timeml_converted/a2/CNN19980227.2130.0067.tml\n",
      "../corpus/timeml_converted/a1/NYT19980206.0460.tml ../corpus/timeml_converted/a2/NYT19980206.0460.tml\n",
      "../corpus/timeml_converted/a1/NYT19980206.0466.tml ../corpus/timeml_converted/a2/NYT19980206.0466.tml\n",
      "../corpus/timeml_converted/a1/NYT19980212.0019.tml ../corpus/timeml_converted/a2/NYT19980212.0019.tml\n",
      "../corpus/timeml_converted/a1/NYT19980402.0453.tml ../corpus/timeml_converted/a2/NYT19980402.0453.tml\n",
      "../corpus/timeml_converted/a1/PRI19980115.2000.0186.tml ../corpus/timeml_converted/a2/PRI19980115.2000.0186.tml\n",
      "../corpus/timeml_converted/a1/PRI19980121.2000.2591.tml ../corpus/timeml_converted/a2/PRI19980121.2000.2591.tml\n",
      "../corpus/timeml_converted/a1/PRI19980205.2000.1890.tml ../corpus/timeml_converted/a2/PRI19980205.2000.1890.tml\n",
      "../corpus/timeml_converted/a1/PRI19980205.2000.1998.tml ../corpus/timeml_converted/a2/PRI19980205.2000.1998.tml\n",
      "../corpus/timeml_converted/a1/PRI19980213.2000.0313.tml ../corpus/timeml_converted/a2/PRI19980213.2000.0313.tml\n",
      "../corpus/timeml_converted/a1/PRI19980216.2000.0170.tml ../corpus/timeml_converted/a2/PRI19980216.2000.0170.tml\n",
      "../corpus/timeml_converted/a1/PRI19980306.2000.1675.tml ../corpus/timeml_converted/a2/PRI19980306.2000.1675.tml\n",
      "../corpus/timeml_converted/a1/ea980120.1830.0071.tml ../corpus/timeml_converted/a2/ea980120.1830.0071.tml\n",
      "../corpus/timeml_converted/a1/ea980120.1830.0456.tml ../corpus/timeml_converted/a2/ea980120.1830.0456.tml\n",
      "../corpus/timeml_converted/a1/ed980111.1130.0089.tml ../corpus/timeml_converted/a2/ed980111.1130.0089.tml\n",
      "Accuracy:  0.7515608591885442\n",
      "Kappa   :  0.6829433287343196\n",
      "Krippendorff alpha:  0.6828686383093623\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "\n",
    "doc2metrics = {}\n",
    "\n",
    "a1_docs = sorted(glob(\"../corpus/timeml_converted/a1/*.tml\"))\n",
    "a2_docs = sorted(glob(\"../corpus/timeml_converted/a2/*.tml\"))\n",
    "\n",
    "assert len(a1_docs) == len(a2_docs)\n",
    "\n",
    "all_relations_a1 = []\n",
    "all_relations_a2 = []\n",
    "\n",
    "for a1_doc, a2_doc in tqdm(zip(a1_docs, a2_docs), total=len(a1_docs)):\n",
    "    print(a1_doc, a2_doc)\n",
    "    with open(a1_doc, \"r\") as f:\n",
    "        soup1 = BeautifulSoup(f.read(), \"xml\")\n",
    "    with open(a2_doc, \"r\") as f:\n",
    "        soup2 = BeautifulSoup(f.read(), \"xml\")\n",
    "    \n",
    "    vocab1 = make_event_vocab(soup1)\n",
    "    vocab2 = make_event_vocab(soup2)\n",
    "    if vocab1 != vocab2:\n",
    "        raise RuntimeError(\n",
    "            f\"Error when processing {a1_doc} and {a2_doc}\\n\"\n",
    "            f\"Vocab1 - Vocab2: {set(vocab1.keys()) - set(vocab2.keys())}\\n\"\n",
    "            f\"Vocab2 - Vocab1: {set(vocab2.keys()) - set(vocab1.keys())}\"\n",
    "        )\n",
    "\n",
    "    graph1, vocab, err = make_graph(soup1)\n",
    "    if err: raise RuntimeError(a1_doc)\n",
    "\n",
    "    graph2, _, err = make_graph(soup2, vocab)\n",
    "    if err: raise RuntimeError(a2_doc)\n",
    "\n",
    "    flat_graph1 = graph1.flatten()\n",
    "    flat_graph2 = graph2.flatten()\n",
    "\n",
    "    all_relations_a1.append(flat_graph1)\n",
    "    all_relations_a2.append(flat_graph2)\n",
    "\n",
    "    doc_id = os.path.basename(a1_doc)\n",
    "    doc2metrics[doc_id] = {\n",
    "        \"accuracy\": metrics.accuracy_score(flat_graph1, flat_graph2),\n",
    "        \"kappa\": metrics.cohen_kappa_score(flat_graph1, flat_graph2),\n",
    "        \"krippendorff\": krippendorff.alpha([flat_graph1, flat_graph2], level_of_measurement=\"nominal\"),\n",
    "    }\n",
    "\n",
    "all_relations_a1 = np.concatenate(all_relations_a1)\n",
    "all_relations_a2 = np.concatenate(all_relations_a2)\n",
    "\n",
    "print(\"Accuracy: \", metrics.accuracy_score(all_relations_a1, all_relations_a2))\n",
    "print(\"Kappa   : \", metrics.cohen_kappa_score(all_relations_a1, all_relations_a2))\n",
    "print(\"Krippendorff alpha: \", krippendorff.alpha([all_relations_a1, all_relations_a2], level_of_measurement=\"nominal\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NYT19980206.0466.tml {'accuracy': 0.4823, 'kappa': 0.3621, 'krippendorff': 0.3495}\n",
      "ed980111.1130.0089.tml {'accuracy': 0.7202, 'kappa': 0.4397, 'krippendorff': 0.4319}\n",
      "NYT19980402.0453.tml {'accuracy': 0.5469, 'kappa': 0.4482, 'krippendorff': 0.4401}\n",
      "NYT19980206.0460.tml {'accuracy': 0.633, 'kappa': 0.5216, 'krippendorff': 0.521}\n",
      "ea980120.1830.0071.tml {'accuracy': 0.6388, 'kappa': 0.5651, 'krippendorff': 0.5604}\n"
     ]
    }
   ],
   "source": [
    "# top5 least-agreement documents (by Krippendorff's alpha) (pretty-printed)\n",
    "least_agreement = sorted(doc2metrics.items(), key=lambda x: x[1][\"krippendorff\"])[:5]\n",
    "for doc_id, doc_metrics in least_agreement:\n",
    "    # round metrics to 4 decimal places\n",
    "    doc_metrics = {k: round(v, 4) for k, v in doc_metrics.items()}\n",
    "    print(doc_id, doc_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"doc2metrics_order.json\", \"w\") as f:\n",
    "    json.dump(doc2metrics, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nt",
   "language": "python",
   "name": "nt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e8bab0656b1eab49efd5cc5b9edf6435da7e0e8161fcfb8a0ce2927c91fd4611"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
